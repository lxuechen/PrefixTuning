#!/bin/bash
mkdir -p /nlp/scr/lxuechen/prefixtune/date_081221/fulltune/distilgpt2-18-576 
CUDA_VISIBLE_DEVICES=0 python -m gpt2stuff.launchers.prefix_vs_full_062021 --mode "local" --tuning_mode fulltune --max_seq_len 100 --nonprivate "no" --per_device_train_batch_size 32 --gradient_accumulation_steps 16 --learning_rate 0.0005 --per_example_max_grad_norm 0.1 --target_epsilon 8 --epochs 50 --eval_epochs 5 --private_engine_mode "ghost"   --model_name_or_path "/home/lxuechen_stanford_edu/dump/distilgpt2/date_080210/distilgpt2-18-576" --max_generations_train 20 --train_dir /nlp/scr/lxuechen/prefixtune/date_081221/fulltune/distilgpt2-18-576 |& tee /nlp/scr/lxuechen/prefixtune/date_081221/fulltune/distilgpt2-18-576/log.out & 
mkdir -p /nlp/scr/lxuechen/prefixtune/date_081221/fulltune/distilgpt2-20-640 
CUDA_VISIBLE_DEVICES=1 python -m gpt2stuff.launchers.prefix_vs_full_062021 --mode "local" --tuning_mode fulltune --max_seq_len 100 --nonprivate "no" --per_device_train_batch_size 32 --gradient_accumulation_steps 16 --learning_rate 0.0005 --per_example_max_grad_norm 0.1 --target_epsilon 8 --epochs 50 --eval_epochs 5 --private_engine_mode "ghost"   --model_name_or_path "/home/lxuechen_stanford_edu/dump/distilgpt2/date_080210/distilgpt2-20-640" --max_generations_train 20 --train_dir /nlp/scr/lxuechen/prefixtune/date_081221/fulltune/distilgpt2-20-640 |& tee /nlp/scr/lxuechen/prefixtune/date_081221/fulltune/distilgpt2-20-640/log.out & 
mkdir -p /nlp/scr/lxuechen/prefixtune/date_081221/fulltune/distilgpt2-22-704 
CUDA_VISIBLE_DEVICES=2 python -m gpt2stuff.launchers.prefix_vs_full_062021 --mode "local" --tuning_mode fulltune --max_seq_len 100 --nonprivate "no" --per_device_train_batch_size 32 --gradient_accumulation_steps 16 --learning_rate 0.0005 --per_example_max_grad_norm 0.1 --target_epsilon 8 --epochs 50 --eval_epochs 5 --private_engine_mode "ghost"   --model_name_or_path "/home/lxuechen_stanford_edu/dump/distilgpt2/date_080210/distilgpt2-22-704" --max_generations_train 20 --train_dir /nlp/scr/lxuechen/prefixtune/date_081221/fulltune/distilgpt2-22-704 |& tee /nlp/scr/lxuechen/prefixtune/date_081221/fulltune/distilgpt2-22-704/log.out & 
wait
mkdir -p /nlp/scr/lxuechen/prefixtune/date_081221/scratchtune/distilgpt2-18-576 
CUDA_VISIBLE_DEVICES=0 python -m gpt2stuff.launchers.prefix_vs_full_062021 --mode "local" --tuning_mode scratchtune --max_seq_len 100 --nonprivate "no" --per_device_train_batch_size 32 --gradient_accumulation_steps 16 --learning_rate 0.0005 --per_example_max_grad_norm 0.1 --target_epsilon 8 --epochs 50 --eval_epochs 5 --private_engine_mode "ghost"   --model_name_or_path "/home/lxuechen_stanford_edu/dump/distilgpt2/date_080210/distilgpt2-18-576" --max_generations_train 20 --train_dir /nlp/scr/lxuechen/prefixtune/date_081221/scratchtune/distilgpt2-18-576 |& tee /nlp/scr/lxuechen/prefixtune/date_081221/scratchtune/distilgpt2-18-576/log.out & 
mkdir -p /nlp/scr/lxuechen/prefixtune/date_081221/scratchtune/distilgpt2-20-640 
CUDA_VISIBLE_DEVICES=1 python -m gpt2stuff.launchers.prefix_vs_full_062021 --mode "local" --tuning_mode scratchtune --max_seq_len 100 --nonprivate "no" --per_device_train_batch_size 32 --gradient_accumulation_steps 16 --learning_rate 0.0005 --per_example_max_grad_norm 0.1 --target_epsilon 8 --epochs 50 --eval_epochs 5 --private_engine_mode "ghost"   --model_name_or_path "/home/lxuechen_stanford_edu/dump/distilgpt2/date_080210/distilgpt2-20-640" --max_generations_train 20 --train_dir /nlp/scr/lxuechen/prefixtune/date_081221/scratchtune/distilgpt2-20-640 |& tee /nlp/scr/lxuechen/prefixtune/date_081221/scratchtune/distilgpt2-20-640/log.out & 
mkdir -p /nlp/scr/lxuechen/prefixtune/date_081221/scratchtune/distilgpt2-22-704 
CUDA_VISIBLE_DEVICES=2 python -m gpt2stuff.launchers.prefix_vs_full_062021 --mode "local" --tuning_mode scratchtune --max_seq_len 100 --nonprivate "no" --per_device_train_batch_size 32 --gradient_accumulation_steps 16 --learning_rate 0.0005 --per_example_max_grad_norm 0.1 --target_epsilon 8 --epochs 50 --eval_epochs 5 --private_engine_mode "ghost"   --model_name_or_path "/home/lxuechen_stanford_edu/dump/distilgpt2/date_080210/distilgpt2-22-704" --max_generations_train 20 --train_dir /nlp/scr/lxuechen/prefixtune/date_081221/scratchtune/distilgpt2-22-704 |& tee /nlp/scr/lxuechen/prefixtune/date_081221/scratchtune/distilgpt2-22-704/log.out & 
wait
